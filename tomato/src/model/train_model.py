import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tqdm import tqdm

# ===================== æ ¸å¿ƒè·¯å¾„é…ç½®ï¼ˆé€‚é…src/model/train_model.pyï¼‰ =====================
# è„šæœ¬å½“å‰è·¯å¾„ï¼šsrc/model/train_model.py
# é¡¹ç›®æ ¹ç›®å½•ï¼šå‘ä¸Šä¸‰çº§ â†’ D:\Project-temp\uav-based-crop-health-cnn\tomato\
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
# é¢„å¤„ç†è¾“å‡ºç›®å½•ï¼štomato/data/processed
PROCESSED_DIR = os.path.join(ROOT_DIR, "data", "processed")
# åŸå§‹æ•°æ®ç›®å½•ï¼štomato/data/raw
RAW_DIR = os.path.join(ROOT_DIR, "data", "raw")

# ===================== è®­ç»ƒå‚æ•°é…ç½® =====================
IMAGE_SIZE = (256, 256)  # ä¸é¢„å¤„ç†ä¸€è‡´çš„å›¾ç‰‡å°ºå¯¸
BATCH_SIZE = 32           # æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´ï¼ˆæ˜¾å­˜ä¸è¶³è®¾ä¸º16/8ï¼‰
EPOCHS = 20               # åŸºç¡€è®­ç»ƒè½®æ•°
LEARNING_RATE = 1e-4      # åŸºç¡€å­¦ä¹ ç‡
NUM_CLASSES = None        # è‡ªåŠ¨è¯†åˆ«ç±»åˆ«æ•°
LABEL_TO_IDX = None       # æ ‡ç­¾ç¼–ç æ˜ å°„å­—å…¸

# ===================== å·¥å…·å‡½æ•°ï¼šåŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›† =====================
def load_and_preprocess_data():
    """
    åŠ è½½train.csv/val.csvï¼Œç”Ÿæˆæ¨¡å‹å¯è®­ç»ƒçš„å›¾ç‰‡æ•°ç»„+One-Hotæ ‡ç­¾
    è¿”å›ï¼šX_train, y_train, X_val, y_val
    """
    global NUM_CLASSES, LABEL_TO_IDX
    
    # 1. åŠ è½½æ ‡ç­¾ç¼–ç æ˜ å°„ï¼ˆä»é¢„å¤„ç†ç›®å½•ï¼‰
    label_to_idx_path = os.path.join(PROCESSED_DIR, "label_to_idx.npy")
    if not os.path.exists(label_to_idx_path):
        raise FileNotFoundError(
            f"âŒ æ ‡ç­¾æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨ï¼š{label_to_idx_path}\n"
            "è¯·å…ˆæ‰§è¡Œï¼špython src/data/prepare_tomato_data.py"
        )
    LABEL_TO_IDX = np.load(label_to_idx_path, allow_pickle=True).item()
    NUM_CLASSES = len(LABEL_TO_IDX)
    print(f"âœ… åŠ è½½æ ‡ç­¾æ˜ å°„å®Œæˆ | ç±»åˆ«æ•°ï¼š{NUM_CLASSES} | æ˜ å°„å…³ç³»ï¼š{LABEL_TO_IDX}")
    
    # 2. åŠ è½½è®­ç»ƒ/éªŒè¯é›†CSVæ–‡ä»¶
    train_csv_path = os.path.join(PROCESSED_DIR, "train.csv")
    val_csv_path = os.path.join(PROCESSED_DIR, "val.csv")
    if not os.path.exists(train_csv_path) or not os.path.exists(val_csv_path):
        raise FileNotFoundError(
            f"âŒ è®­ç»ƒ/éªŒè¯é›†CSVä¸å­˜åœ¨\n"
            f"ç¼ºå¤±æ–‡ä»¶ï¼š{train_csv_path if not os.path.exists(train_csv_path) else val_csv_path}\n"
            "è¯·å…ˆæ‰§è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬"
        )
    
    train_df = pd.read_csv(train_csv_path)
    val_df = pd.read_csv(val_csv_path)
    print(f"âœ… åŠ è½½æ•°æ®é›†å®Œæˆ | è®­ç»ƒé›†ï¼š{len(train_df)} å¼  | éªŒè¯é›†ï¼š{len(val_df)} å¼ ")
    
    # 3. åŠ è½½å¹¶é¢„å¤„ç†å›¾ç‰‡ï¼ˆç»Ÿä¸€è·¯å¾„æ‹¼æ¥é€»è¾‘ï¼‰
    def process_dataframe(df, desc):
        """å¤„ç†å•ä¸ªDataFrameï¼Œè¿”å›å›¾ç‰‡æ•°ç»„å’ŒOne-Hotæ ‡ç­¾"""
        images = []
        labels = []
        for idx, row in tqdm(df.iterrows(), desc=desc, total=len(df)):
            # æ‹¼æ¥å›¾ç‰‡ç»å¯¹è·¯å¾„ï¼šé¡¹ç›®æ ¹ç›®å½• + ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚data/raw/Tomato___Healthy/xxx.jpgï¼‰
            img_rel_path = row["img_path"]
            img_abs_path = os.path.join(ROOT_DIR, img_rel_path)
            
            # æ ¡éªŒå›¾ç‰‡æ˜¯å¦å­˜åœ¨
            if not os.path.exists(img_abs_path):
                print(f"âš ï¸  å›¾ç‰‡ä¸å­˜åœ¨ï¼Œè·³è¿‡ï¼š{img_abs_path}")
                continue
            
            # åŠ è½½å›¾ç‰‡å¹¶é¢„å¤„ç†
            img = load_img(img_abs_path, target_size=IMAGE_SIZE)  # è°ƒæ•´å°ºå¯¸
            img_array = img_to_array(img) / 255.0  # å½’ä¸€åŒ–åˆ°0-1
            
            images.append(img_array)
            labels.append(row["label"])
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„ + One-Hotç¼–ç æ ‡ç­¾
        images = np.array(images, dtype=np.float32)
        labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)
        
        return images, labels
    
    # å¤„ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    X_train, y_train = process_dataframe(train_df, "åŠ è½½è®­ç»ƒé›†å›¾ç‰‡")
    X_val, y_val = process_dataframe(val_df, "åŠ è½½éªŒè¯é›†å›¾ç‰‡")
    
    # è¾“å‡ºæ•°æ®ç»´åº¦ä¿¡æ¯
    print(f"âœ… å›¾ç‰‡é¢„å¤„ç†å®Œæˆï¼š")
    print(f"   - è®­ç»ƒé›†ï¼š{X_train.shape} | æ ‡ç­¾ï¼š{y_train.shape}")
    print(f"   - éªŒè¯é›†ï¼š{X_val.shape} | æ ‡ç­¾ï¼š{y_val.shape}")
    
    return X_train, y_train, X_val, y_val

# ===================== å·¥å…·å‡½æ•°ï¼šæ„å»ºè¿ç§»å­¦ä¹ æ¨¡å‹ =====================
def build_tomato_model():
    """æ„å»ºåŸºäºMobileNetV2çš„è¿ç§»å­¦ä¹ æ¨¡å‹ï¼Œé€‚é…256Ã—256å°ºå¯¸"""
    # 1. åŠ è½½é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œï¼ˆå†»ç»“åº•å±‚æƒé‡ï¼‰
    base_model = MobileNetV2(
        input_shape=(*IMAGE_SIZE, 3),
        weights="imagenet",  # ä½¿ç”¨ImageNeté¢„è®­ç»ƒæƒé‡
        include_top=False     # ä¸åŒ…å«é¡¶å±‚åˆ†ç±»å™¨
    )
    base_model.trainable = False  # å…ˆå†»ç»“ï¼Œè®­ç»ƒåæœŸå¾®è°ƒ
    
    # 2. æ„å»ºå®Œæ•´æ¨¡å‹ï¼ˆæ•°æ®å¢å¼º + ç‰¹å¾æå– + åˆ†ç±»ï¼‰
    model = models.Sequential([
        # æ•°æ®å¢å¼ºå±‚ï¼ˆä»…è®­ç»ƒé˜¶æ®µç”Ÿæ•ˆï¼‰
        layers.RandomFlip("horizontal", input_shape=(*IMAGE_SIZE, 3)),
        layers.RandomRotation(0.15),
        layers.RandomZoom(0.15),
        layers.RandomContrast(0.1),
        
        # é¢„è®­ç»ƒéª¨å¹²
        base_model,
        
        # ç‰¹å¾èšåˆä¸åˆ†ç±»
        layers.GlobalAveragePooling2D(),  # å…¨å±€å¹³å‡æ± åŒ–ï¼Œé™ä½å‚æ•°é‡
        layers.Dense(256, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(1e-4)),
        layers.Dropout(0.5),  # Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
        layers.Dense(NUM_CLASSES, activation="softmax")  # åˆ†ç±»è¾“å‡ºå±‚
    ])
    
    # 3. ç¼–è¯‘æ¨¡å‹
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss="categorical_crossentropy",
        metrics=["accuracy"]
    )
    
    # æ‰“å°æ¨¡å‹ç»“æ„
    print("\nğŸ“Œ æ¨¡å‹ç»“æ„æ¦‚è§ˆï¼š")
    model.summary(expand_nested=True)
    
    return model, base_model

# ===================== å·¥å…·å‡½æ•°ï¼šæ‰§è¡Œæ¨¡å‹è®­ç»ƒ =====================
def train_tomato_model():
    """ä¸»è®­ç»ƒæµç¨‹ï¼šåŠ è½½æ•°æ® â†’ æ„å»ºæ¨¡å‹ â†’ è®­ç»ƒ â†’ å¾®è°ƒ â†’ ä¿å­˜"""
    # 1. åŠ è½½é¢„å¤„ç†æ•°æ®
    X_train, y_train, X_val, y_val = load_and_preprocess_data()
    
    # 2. æ„å»ºæ¨¡å‹
    model, base_model = build_tomato_model()
    
    # 3. å®šä¹‰è®­ç»ƒå›è°ƒå‡½æ•°
    callbacks = [
        # æ—©åœï¼šéªŒè¯é›†ç²¾åº¦5è½®ä¸æå‡åˆ™åœæ­¢ï¼Œæ¢å¤æœ€ä¼˜æƒé‡
        tf.keras.callbacks.EarlyStopping(
            monitor="val_accuracy",
            patience=5,
            restore_best_weights=True,
            verbose=1
        ),
        # æ¨¡å‹ä¿å­˜ï¼šä¿å­˜éªŒè¯é›†ç²¾åº¦æœ€é«˜çš„æ¨¡å‹
        tf.keras.callbacks.ModelCheckpoint(
            filepath=os.path.join(PROCESSED_DIR, "best_tomato_model.keras"),
            monitor="val_accuracy",
            save_best_only=True,
            verbose=1
        ),
        # å­¦ä¹ ç‡è°ƒåº¦ï¼šéªŒè¯é›†æŸå¤±ä¸ä¸‹é™åˆ™é™ä½å­¦ä¹ ç‡
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor="val_loss",
            factor=0.5,
            patience=3,
            min_lr=1e-6,
            verbose=1
        ),
        # TensorBoardæ—¥å¿—ï¼ˆæ–¹ä¾¿å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ï¼‰
        tf.keras.callbacks.TensorBoard(
            log_dir=os.path.join(ROOT_DIR, "logs", "tomato_model"),
            histogram_freq=1
        )
    ]
    
    # 4. åŸºç¡€è®­ç»ƒï¼ˆå†»ç»“é¢„è®­ç»ƒå±‚ï¼‰
    print("\nğŸš€ å¼€å§‹åŸºç¡€è®­ç»ƒï¼ˆå†»ç»“é¢„è®­ç»ƒå±‚ï¼‰...")
    history_base = model.fit(
        X_train, y_train,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=1
    )
    
    # 5. æ¨¡å‹å¾®è°ƒï¼ˆè§£å†»é¢„è®­ç»ƒå±‚é¡¶å±‚ï¼Œæå‡ç²¾åº¦ï¼‰
    print("\nğŸ”§ å¼€å§‹æ¨¡å‹å¾®è°ƒï¼ˆè§£å†»MobileNetV2é¡¶å±‚ï¼‰...")
    base_model.trainable = True
    # åªè§£å†»é¡¶å±‚20å±‚ï¼Œåº•å±‚ä¿ç•™é¢„è®­ç»ƒç‰¹å¾
    fine_tune_at = len(base_model.layers) - 20
    for layer in base_model.layers[:fine_tune_at]:
        layer.trainable = False
    
    # é‡æ–°ç¼–è¯‘ï¼ˆé™ä½å­¦ä¹ ç‡ï¼Œé¿å…ç ´åé¢„è®­ç»ƒæƒé‡ï¼‰
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
        loss="categorical_crossentropy",
        metrics=["accuracy"]
    )
    
    # ç»§ç»­å¾®è°ƒè®­ç»ƒ
    history_fine = model.fit(
        X_train, y_train,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS + 10,  # é¢å¤–è®­ç»ƒ10è½®
        initial_epoch=history_base.epoch[-1],
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=1
    )
    
    # 6. åˆå¹¶è®­ç»ƒå†å²å¹¶ä¿å­˜
    all_history = {
        "loss": history_base.history["loss"] + history_fine.history["loss"],
        "val_loss": history_base.history["val_loss"] + history_fine.history["val_loss"],
        "accuracy": history_base.history["accuracy"] + history_fine.history["accuracy"],
        "val_accuracy": history_base.history["val_accuracy"] + history_fine.history["val_accuracy"]
    }
    history_df = pd.DataFrame(all_history)
    history_df.to_csv(os.path.join(PROCESSED_DIR, "train_history.csv"), index=False)
    
    # 7. æœ€ç»ˆè¯„ä¼°
    print("\nğŸ“Š è®­ç»ƒå®Œæˆ | æœ€ç»ˆéªŒè¯é›†è¯„ä¼°ï¼š")
    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
    print(f"   - éªŒè¯é›†ç²¾åº¦ï¼š{val_acc:.4f}")
    print(f"   - éªŒè¯é›†æŸå¤±ï¼š{val_loss:.4f}")
    
    # è¾“å‡ºç”Ÿæˆæ–‡ä»¶è·¯å¾„
    print("\nğŸ“ ç”Ÿæˆæ–‡ä»¶æ¸…å•ï¼š")
    print(f"   âœ… æœ€ä½³æ¨¡å‹ï¼š{os.path.join(PROCESSED_DIR, 'best_tomato_model.keras')}")
    print(f"   âœ… è®­ç»ƒå†å²ï¼š{os.path.join(PROCESSED_DIR, 'train_history.csv')}")
    print(f"   âœ… TensorBoardæ—¥å¿—ï¼š{os.path.join(ROOT_DIR, 'logs', 'tomato_model')}")
    
    return model, history_base, history_fine

# ===================== ä¸»å‡½æ•°ï¼šå¯åŠ¨è®­ç»ƒ =====================
if __name__ == "__main__":
    # å‰ç½®æ ¡éªŒï¼šé¢„å¤„ç†ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(PROCESSED_DIR):
        raise FileNotFoundError(
            f"âŒ é¢„å¤„ç†ç›®å½•ä¸å­˜åœ¨ï¼š{PROCESSED_DIR}\n"
            "è¯·å…ˆæ‰§è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬ï¼špython src/data/prepare_tomato_data.py"
        )
    
    # æ‰§è¡Œè®­ç»ƒ
    try:
        model, history_base, history_fine = train_tomato_model()
        print("\nğŸ‰ ç•ªèŒ„èƒè¿«è¯†åˆ«æ¨¡å‹è®­ç»ƒå…¨éƒ¨å®Œæˆï¼")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™ï¼š{str(e)}")
        raise  # æŠ›å‡ºå¼‚å¸¸ï¼Œä¾¿äºå®šä½é—®é¢˜