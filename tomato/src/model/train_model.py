import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tqdm import tqdm

# ===================== æ ¸å¿ƒè·¯å¾„é…ç½® =====================
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
PROCESSED_DIR = os.path.join(ROOT_DIR, "data", "processed")
RAW_DIR = os.path.join(ROOT_DIR, "data", "raw")

# ===================== è®­ç»ƒå‚æ•°é…ç½®ï¼ˆä¼˜åŒ–åï¼‰=====================
IMAGE_SIZE = (256, 256)
BATCH_SIZE = 32
BASE_EPOCHS = 60  # åŸºç¡€è®­ç»ƒè½®æ¬¡ä»20å¢åŠ åˆ°60
FINE_TUNE_EPOCHS = 40  # å¾®è°ƒè½®æ¬¡ä»10å¢åŠ åˆ°40ï¼ˆæ€»è½®æ¬¡100ï¼‰
LEARNING_RATE = 1e-4
FINE_TUNE_LR = 1e-5  # å¾®è°ƒåˆå§‹å­¦ä¹ ç‡
NUM_CLASSES = None
LABEL_TO_IDX = None

# ===================== å·¥å…·å‡½æ•°ï¼šåŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›† =====================
def load_and_preprocess_data():
    # ä¿æŒåŸæœ‰å®ç°ä¸å˜
    global NUM_CLASSES, LABEL_TO_IDX
    
    label_to_idx_path = os.path.join(PROCESSED_DIR, "label_to_idx.npy")
    if not os.path.exists(label_to_idx_path):
        raise FileNotFoundError(
            f"âŒ æ ‡ç­¾æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨ï¼š{label_to_idx_path}\n"
            "è¯·å…ˆæ‰§è¡Œï¼špython src/data/prepare_tomato_data.py"
        )
    LABEL_TO_IDX = np.load(label_to_idx_path, allow_pickle=True).item()
    NUM_CLASSES = len(LABEL_TO_IDX)
    print(f"âœ… åŠ è½½æ ‡ç­¾æ˜ å°„å®Œæˆ | ç±»åˆ«æ•°ï¼š{NUM_CLASSES} | æ˜ å°„å…³ç³»ï¼š{LABEL_TO_IDX}")
    
    train_csv_path = os.path.join(PROCESSED_DIR, "train.csv")
    val_csv_path = os.path.join(PROCESSED_DIR, "val.csv")
    if not os.path.exists(train_csv_path) or not os.path.exists(val_csv_path):
        raise FileNotFoundError(
            f"âŒ è®­ç»ƒ/éªŒè¯é›†CSVä¸å­˜åœ¨\n"
            f"ç¼ºå¤±æ–‡ä»¶ï¼š{train_csv_path if not os.path.exists(train_csv_path) else val_csv_path}\n"
            "è¯·å…ˆæ‰§è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬"
        )
    
    train_df = pd.read_csv(train_csv_path)
    val_df = pd.read_csv(val_csv_path)
    print(f"âœ… åŠ è½½æ•°æ®é›†å®Œæˆ | è®­ç»ƒé›†ï¼š{len(train_df)} å¼  | éªŒè¯é›†ï¼š{len(val_df)} å¼ ")
    
    def process_dataframe(df, desc):
        images = []
        labels = []
        for idx, row in tqdm(df.iterrows(), desc=desc, total=len(df)):
            img_rel_path = row["img_path"]
            img_abs_path = os.path.join(ROOT_DIR, img_rel_path)
            
            if not os.path.exists(img_abs_path):
                print(f"âš ï¸  å›¾ç‰‡ä¸å­˜åœ¨ï¼Œè·³è¿‡ï¼š{img_abs_path}")
                continue
            
            img = load_img(img_abs_path, target_size=IMAGE_SIZE)
            img_array = img_to_array(img) / 255.0
            
            images.append(img_array)
            labels.append(row["label"])
        
        images = np.array(images, dtype=np.float32)
        labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)
        
        return images, labels
    
    X_train, y_train = process_dataframe(train_df, "åŠ è½½è®­ç»ƒé›†å›¾ç‰‡")
    X_val, y_val = process_dataframe(val_df, "åŠ è½½éªŒè¯é›†å›¾ç‰‡")
    
    print(f"âœ… å›¾ç‰‡é¢„å¤„ç†å®Œæˆï¼š")
    print(f"   - è®­ç»ƒé›†ï¼š{X_train.shape} | æ ‡ç­¾ï¼š{y_train.shape}")
    print(f"   - éªŒè¯é›†ï¼š{X_val.shape} | æ ‡ç­¾ï¼š{y_val.shape}")
    
    return X_train, y_train, X_val, y_val

# ===================== å·¥å…·å‡½æ•°ï¼šæ„å»ºè¿ç§»å­¦ä¹ æ¨¡å‹ï¼ˆå¢å¼ºç‰ˆï¼‰=====================
def build_tomato_model():
    base_model = MobileNetV2(
        input_shape=(*IMAGE_SIZE, 3),
        weights="imagenet",
        include_top=False
    )
    base_model.trainable = False
    
    # å¢å¼ºæ•°æ®å¢å¼ºç­–ç•¥
    model = models.Sequential([
        layers.RandomFlip("horizontal_and_vertical", input_shape=(*IMAGE_SIZE, 3)),  # å¢åŠ å‚ç›´ç¿»è½¬
        layers.RandomRotation(0.2),  # å¢å¤§æ—‹è½¬è§’åº¦
        layers.RandomZoom(0.2, 0.2),  # å¢å¤§ç¼©æ”¾èŒƒå›´
        layers.RandomContrast(0.2),  # å¢å¤§å¯¹æ¯”åº¦è°ƒæ•´èŒƒå›´
        layers.RandomTranslation(0.1, 0.1),  # æ–°å¢å¹³ç§»å˜æ¢
        
        base_model,
        
        layers.GlobalAveragePooling2D(),
        layers.Dense(512, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(1e-4)),  # å¢åŠ å…¨è¿æ¥å±‚ç»´åº¦
        layers.BatchNormalization(),  # æ–°å¢æ‰¹å½’ä¸€åŒ–å±‚
        layers.Dropout(0.6),  # æé«˜dropoutæ¯”ä¾‹
        layers.Dense(NUM_CLASSES, activation="softmax")
    ])
    
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss="categorical_crossentropy",
        metrics=["accuracy", tf.keras.metrics.Precision(name='precision'), 
                 tf.keras.metrics.Recall(name='recall')]  # å¢åŠ è¯„ä¼°æŒ‡æ ‡
    )
    
    print("\nğŸ“Œ æ¨¡å‹ç»“æ„æ¦‚è§ˆï¼š")
    model.summary(expand_nested=True)
    
    return model, base_model

# ===================== å·¥å…·å‡½æ•°ï¼šæ‰§è¡Œæ¨¡å‹è®­ç»ƒï¼ˆå»¶é•¿è®­ç»ƒæ—¶é—´ï¼‰=====================
def train_tomato_model():
    X_train, y_train, X_val, y_val = load_and_preprocess_data()
    model, base_model = build_tomato_model()
    
    # ä¼˜åŒ–å›è°ƒå‡½æ•°
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            monitor="val_accuracy",
            patience=15,  # å»¶é•¿æ—©åœè€å¿ƒå€¼ï¼Œé¿å…è¿‡æ—©åœæ­¢
            restore_best_weights=True,
            verbose=1
        ),
        tf.keras.callbacks.ModelCheckpoint(
            filepath=os.path.join(PROCESSED_DIR, "best_tomato_model.keras"),
            monitor="val_accuracy",
            save_best_only=True,
            verbose=1
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor="val_loss",
            factor=0.5,
            patience=5,  # å»¶é•¿å­¦ä¹ ç‡è°ƒæ•´è€å¿ƒå€¼
            min_lr=1e-7,  # é™ä½æœ€å°å­¦ä¹ ç‡
            verbose=1
        ),
        tf.keras.callbacks.TensorBoard(
            log_dir=os.path.join(ROOT_DIR, "logs", "tomato_model"),
            histogram_freq=1
        )
    ]
    
    # åŸºç¡€è®­ç»ƒï¼ˆå»¶é•¿è‡³60è½®ï¼‰
    print("\nğŸš€ å¼€å§‹åŸºç¡€è®­ç»ƒï¼ˆå†»ç»“é¢„è®­ç»ƒå±‚ï¼‰...")
    history_base = model.fit(
        X_train, y_train,
        batch_size=BATCH_SIZE,
        epochs=BASE_EPOCHS,
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=1
    )
    
    # æ¨¡å‹å¾®è°ƒï¼ˆå»¶é•¿è‡³40è½®ï¼‰
    print("\nğŸ”§ å¼€å§‹æ¨¡å‹å¾®è°ƒï¼ˆè§£å†»æ›´å¤šå±‚ï¼‰...")
    base_model.trainable = True
    # è§£å†»æ›´å¤šå±‚ï¼ˆä»å€’æ•°30å±‚å¼€å§‹ï¼‰
    fine_tune_at = len(base_model.layers) - 30
    for layer in base_model.layers[:fine_tune_at]:
        layer.trainable = False
    
    # å¾®è°ƒé˜¶æ®µä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
    fine_tune_optimizer = tf.keras.optimizers.Adam(
        learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(
            initial_learning_rate=FINE_TUNE_LR,
            decay_steps=10000,
            decay_rate=0.9
        )
    )
    
    model.compile(
        optimizer=fine_tune_optimizer,
        loss="categorical_crossentropy",
        metrics=["accuracy", tf.keras.metrics.Precision(name='precision'), 
                 tf.keras.metrics.Recall(name='recall')]
    )
    
    # ç»§ç»­å¾®è°ƒè®­ç»ƒï¼ˆæ€»è½®æ¬¡ = åŸºç¡€è½®æ¬¡ + å¾®è°ƒè½®æ¬¡ï¼‰
    history_fine = model.fit(
        X_train, y_train,
        batch_size=BATCH_SIZE,
        epochs=BASE_EPOCHS + FINE_TUNE_EPOCHS,
        initial_epoch=history_base.epoch[-1],
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=1
    )
    
    # åˆå¹¶è®­ç»ƒå†å²
    all_history = {
        "loss": history_base.history["loss"] + history_fine.history["loss"],
        "val_loss": history_base.history["val_loss"] + history_fine.history["val_loss"],
        "accuracy": history_base.history["accuracy"] + history_fine.history["accuracy"],
        "val_accuracy": history_base.history["val_accuracy"] + history_fine.history["val_accuracy"]
    }
    history_df = pd.DataFrame(all_history)
    history_df.to_csv(os.path.join(PROCESSED_DIR, "train_history.csv"), index=False)
    
    # æœ€ç»ˆè¯„ä¼°
    print("\nğŸ“Š è®­ç»ƒå®Œæˆ | æœ€ç»ˆéªŒè¯é›†è¯„ä¼°ï¼š")
    val_loss, val_acc, val_precision, val_recall = model.evaluate(X_val, y_val, verbose=0)
    print(f"   - éªŒè¯é›†ç²¾åº¦ï¼š{val_acc:.4f}")
    print(f"   - éªŒè¯é›†æŸå¤±ï¼š{val_loss:.4f}")
    print(f"   - éªŒè¯é›†ç²¾ç¡®ç‡ï¼š{val_precision:.4f}")
    print(f"   - éªŒè¯é›†å¬å›ç‡ï¼š{val_recall:.4f}")
    
    print("\nğŸ“ ç”Ÿæˆæ–‡ä»¶æ¸…å•ï¼š")
    print(f"   âœ… æœ€ä½³æ¨¡å‹ï¼š{os.path.join(PROCESSED_DIR, 'best_tomato_model.keras')}")
    print(f"   âœ… è®­ç»ƒå†å²ï¼š{os.path.join(PROCESSED_DIR, 'train_history.csv')}")
    print(f"   âœ… TensorBoardæ—¥å¿—ï¼š{os.path.join(ROOT_DIR, 'logs', 'tomato_model')}")
    
    return model, history_base, history_fine

# ===================== ä¸»å‡½æ•°ï¼šå¯åŠ¨è®­ç»ƒ =====================
if __name__ == "__main__":
    if not os.path.exists(PROCESSED_DIR):
        raise FileNotFoundError(
            f"âŒ é¢„å¤„ç†ç›®å½•ä¸å­˜åœ¨ï¼š{PROCESSED_DIR}\n"
            "è¯·å…ˆæ‰§è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬ï¼špython src/data/prepare_tomato_data.py"
        )
    
    try:
        model, history_base, history_fine = train_tomato_model()
        print("\nğŸ‰ ç•ªèŒ„èƒè¿«è¯†åˆ«æ¨¡å‹è®­ç»ƒå…¨éƒ¨å®Œæˆï¼")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™ï¼š{str(e)}")
        raise