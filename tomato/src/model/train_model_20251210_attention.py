# ==============================================================================
# Tomato Leaf Disease Recognition Model with Attention Mechanism (v2.0)
# File: train_model_20251210_attention.py
# Path: src/model/train_model_20251210_attention.py
# Features: 
# 1. Integrated CBAM Attention for Image Feature Extraction
# 2. Enhanced Data Augmentation for Plant Leaf Images
# 3. AdamW Optimizer with Explicit Weight Decay
# 4. Progressive Fine-tuning Strategy
# 5. Comprehensive Metrics (Accuracy/Precision/Recall)
# ==============================================================================

import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, backend as K
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tqdm import tqdm

# ===================== æ ¸å¿ƒè·¯å¾„é…ç½® =====================
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
PROCESSED_DIR = os.path.join(ROOT_DIR, "data", "processed")
RAW_DIR = os.path.join(ROOT_DIR, "data", "raw")

# ===================== è®­ç»ƒå‚æ•°é…ç½®ï¼ˆä¼˜åŒ–åï¼‰=====================
IMAGE_SIZE = (256, 256)
BATCH_SIZE = 32
BASE_EPOCHS = 60  # åŸºç¡€è®­ç»ƒè½®æ¬¡
FINE_TUNE_EPOCHS = 100  # å¾®è°ƒè½®æ¬¡
LEARNING_RATE = 1e-4
FINE_TUNE_LR = 1e-5  # å¾®è°ƒåˆå§‹å­¦ä¹ ç‡
NUM_CLASSES = None
LABEL_TO_IDX = None

# ===================== å›¾åƒæ³¨æ„åŠ›æœºåˆ¶ï¼šCBAMç®€åŒ–ç‰ˆï¼ˆé€‚é…CNNï¼‰=====================
class ChannelAttention(layers.Layer):
    """é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼ˆSE Attentionï¼‰ï¼Œé€‚é…å›¾åƒç‰¹å¾"""
    def __init__(self, ratio=16, **kwargs):
        super().__init__(**kwargs)
        self.ratio = ratio

    def build(self, input_shape):
        self.channels = input_shape[-1]
        self.fc1 = layers.Dense(self.channels // self.ratio, activation="relu")
        self.fc2 = layers.Dense(self.channels, activation="sigmoid")
        super().build(input_shape)

    def call(self, x):
        # å…¨å±€å¹³å‡æ± åŒ– + å…¨å±€æœ€å¤§æ± åŒ–
        avg_pool = layers.GlobalAveragePooling2D()(x)
        max_pool = layers.GlobalMaxPooling2D()(x)
        
        # å…¨è¿æ¥å±‚æå–é€šé“æ³¨æ„åŠ›æƒé‡
        avg_out = self.fc2(self.fc1(avg_pool))
        max_out = self.fc2(self.fc1(max_pool))
        
        # æƒé‡èåˆ + ç‰¹å¾åŠ æƒ
        attention = layers.Add()([avg_out, max_out])
        attention = layers.Reshape((1, 1, self.channels))(attention)
        return x * attention

    def compute_output_shape(self, input_shape):
        return input_shape

class SpatialAttention(layers.Layer):
    """ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€‚é…å›¾åƒç‰¹å¾"""
    def __init__(self, kernel_size=7, **kwargs):
        super().__init__(**kwargs)
        self.kernel_size = kernel_size

    def build(self, input_shape):
        self.conv = layers.Conv2D(1, self.kernel_size, padding="same", activation="sigmoid")
        super().build(input_shape)

    def call(self, x):
        # é€šé“ç»´åº¦çš„å¹³å‡æ± åŒ– + æœ€å¤§æ± åŒ–
        avg_pool = layers.Lambda(lambda x: K.mean(x, axis=-1, keepdims=True))(x)
        max_pool = layers.Lambda(lambda x: K.max(x, axis=-1, keepdims=True))(x)
        
        # å·ç§¯æå–ç©ºé—´æ³¨æ„åŠ›æƒé‡
        concat = layers.Concatenate(axis=-1)([avg_pool, max_pool])
        attention = self.conv(concat)
        
        # ç‰¹å¾åŠ æƒ
        return x * attention

    def compute_output_shape(self, input_shape):
        return input_shape

class CBAMAttention(layers.Layer):
    """CBAMæ³¨æ„åŠ›ï¼ˆé€šé“+ç©ºé—´ï¼‰ï¼Œé€‚é…CNNå›¾åƒç‰¹å¾æå–"""
    def __init__(self, ratio=16, kernel_size=7, **kwargs):
        super().__init__(**kwargs)
        self.channel_att = ChannelAttention(ratio)
        self.spatial_att = SpatialAttention(kernel_size)

    def call(self, x):
        x = self.channel_att(x)
        x = self.spatial_att(x)
        return x

# ===================== å·¥å…·å‡½æ•°ï¼šåŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›† =====================
def load_and_preprocess_data():
    global NUM_CLASSES, LABEL_TO_IDX
    
    label_to_idx_path = os.path.join(PROCESSED_DIR, "label_to_idx.npy")
    if not os.path.exists(label_to_idx_path):
        raise FileNotFoundError(
            f"âŒ æ ‡ç­¾æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨ï¼š{label_to_idx_path}\n"
            "è¯·å…ˆæ‰§è¡Œï¼špython src/data/prepare_tomato_data.py"
        )
    LABEL_TO_IDX = np.load(label_to_idx_path, allow_pickle=True).item()
    NUM_CLASSES = len(LABEL_TO_IDX)
    print(f"âœ… åŠ è½½æ ‡ç­¾æ˜ å°„å®Œæˆ | ç±»åˆ«æ•°ï¼š{NUM_CLASSES} | æ˜ å°„å…³ç³»ï¼š{LABEL_TO_IDX}")
    
    train_csv_path = os.path.join(PROCESSED_DIR, "train.csv")
    val_csv_path = os.path.join(PROCESSED_DIR, "val.csv")
    if not os.path.exists(train_csv_path) or not os.path.exists(val_csv_path):
        raise FileNotFoundError(
            f"âŒ è®­ç»ƒ/éªŒè¯é›†CSVä¸å­˜åœ¨\n"
            f"ç¼ºå¤±æ–‡ä»¶ï¼š{train_csv_path if not os.path.exists(train_csv_path) else val_csv_path}\n"
            "è¯·å…ˆæ‰§è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬"
        )
    
    train_df = pd.read_csv(train_csv_path)
    val_df = pd.read_csv(val_csv_path)
    print(f"âœ… åŠ è½½æ•°æ®é›†å®Œæˆ | è®­ç»ƒé›†ï¼š{len(train_df)} å¼  | éªŒè¯é›†ï¼š{len(val_df)} å¼ ")
    
    def process_dataframe(df, desc):
        images = []
        labels = []
        for idx, row in tqdm(df.iterrows(), desc=desc, total=len(df)):
            img_rel_path = row["img_path"]
            img_abs_path = os.path.join(ROOT_DIR, img_rel_path)
            
            if not os.path.exists(img_abs_path):
                print(f"âš ï¸  å›¾ç‰‡ä¸å­˜åœ¨ï¼Œè·³è¿‡ï¼š{img_abs_path}")
                continue
            
            img = load_img(img_abs_path, target_size=IMAGE_SIZE)
            img_array = img_to_array(img) / 255.0
            
            images.append(img_array)
            labels.append(row["label"])
        
        images = np.array(images, dtype=np.float32)
        labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)
        
        return images, labels
    
    X_train, y_train = process_dataframe(train_df, "åŠ è½½è®­ç»ƒé›†å›¾ç‰‡")
    X_val, y_val = process_dataframe(val_df, "åŠ è½½éªŒè¯é›†å›¾ç‰‡")
    
    print(f"âœ… å›¾ç‰‡é¢„å¤„ç†å®Œæˆï¼š")
    print(f"   - è®­ç»ƒé›†ï¼š{X_train.shape} | æ ‡ç­¾ï¼š{y_train.shape}")
    print(f"   - éªŒè¯é›†ï¼š{X_val.shape} | æ ‡ç­¾ï¼š{y_val.shape}")
    
    return X_train, y_train, X_val, y_val

# ===================== å·¥å…·å‡½æ•°ï¼šæ„å»ºè¿ç§»å­¦ä¹ æ¨¡å‹ï¼ˆä¿®æ­£æ³¨æ„åŠ›æœºåˆ¶ï¼‰=====================
def build_tomato_model():
    base_model = MobileNetV2(
        input_shape=(*IMAGE_SIZE, 3),
        weights="imagenet",
        include_top=False
    )
    base_model.trainable = False
    
    # å¢å¼ºæ•°æ®å¢å¼ºç­–ç•¥
    model = models.Sequential([
        # æ•°æ®å¢å¼ºå±‚
        layers.RandomFlip("horizontal_and_vertical", input_shape=(*IMAGE_SIZE, 3)),
        layers.RandomRotation(0.2),
        layers.RandomZoom(0.2, 0.2),
        layers.RandomContrast(0.2),
        layers.RandomTranslation(0.1, 0.1),
        
        # åŸºç¡€ç‰¹å¾æå–
        base_model,
        
        # æ ¸å¿ƒæ”¹è¿›ï¼šCBAMæ³¨æ„åŠ›æœºåˆ¶ï¼ˆé€‚é…å›¾åƒCNNï¼‰
        CBAMAttention(ratio=16, kernel_size=7),
        
        # ç‰¹å¾èšåˆä¸åˆ†ç±»å¤´
        layers.GlobalAveragePooling2D(),
        layers.Dense(512, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(1e-4)),
        layers.BatchNormalization(),
        layers.Dropout(0.6),
        layers.Dense(NUM_CLASSES, activation="softmax")
    ])
    
    # ä½¿ç”¨AdamWä¼˜åŒ–å™¨å¢å¼ºæ­£åˆ™åŒ–æ•ˆæœ
    optimizer = tf.keras.optimizers.AdamW(
        learning_rate=LEARNING_RATE,
        weight_decay=1e-4
    )
    
    model.compile(
        optimizer=optimizer,
        loss="categorical_crossentropy",
        metrics=["accuracy", tf.keras.metrics.Precision(name='precision'), 
                 tf.keras.metrics.Recall(name='recall')]
    )
    
    print("\nğŸ“Œ æ¨¡å‹ç»“æ„æ¦‚è§ˆï¼ˆv2.0 | å«CBAMæ³¨æ„åŠ›æœºåˆ¶ï¼‰ï¼š")
    model.summary(expand_nested=True)
    
    return model, base_model

# ===================== å·¥å…·å‡½æ•°ï¼šæ‰§è¡Œæ¨¡å‹è®­ç»ƒ =====================
def train_tomato_model():
    X_train, y_train, X_val, y_val = load_and_preprocess_data()
    model, base_model = build_tomato_model()
    
    # ä¼˜åŒ–å›è°ƒå‡½æ•°ï¼ˆä¿ç•™åŸæ–‡ä»¶åï¼‰
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            monitor="val_accuracy",
            patience=20,
            restore_best_weights=True,
            verbose=1
        ),
        tf.keras.callbacks.ModelCheckpoint(
            filepath=os.path.join(PROCESSED_DIR, "best_tomato_model.keras"),
            monitor="val_accuracy",
            save_best_only=True,
            verbose=1
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor="val_loss",
            factor=0.5,
            patience=5,
            min_lr=1e-7,
            verbose=1
        ),
        tf.keras.callbacks.TensorBoard(
            log_dir=os.path.join(ROOT_DIR, "logs", "tomato_model"),
            histogram_freq=1
        )
    ]
    
    # åŸºç¡€è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹åŸºç¡€è®­ç»ƒï¼ˆv2.0 | å†»ç»“é¢„è®­ç»ƒå±‚ï¼‰...")
    history_base = model.fit(
        X_train, y_train,
        batch_size=BATCH_SIZE,
        epochs=BASE_EPOCHS,
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=1
    )
    
    # æ¨¡å‹å¾®è°ƒ
    print("\nğŸ”§ å¼€å§‹æ¨¡å‹å¾®è°ƒï¼ˆv2.0 | è§£å†»æ›´å¤šå±‚ï¼‰...")
    base_model.trainable = True
    fine_tune_at = len(base_model.layers) - 30
    for layer in base_model.layers[:fine_tune_at]:
        layer.trainable = False
    
    # å¾®è°ƒé˜¶æ®µä¼˜åŒ–å™¨ï¼ˆå¸¦å­¦ä¹ ç‡è¡°å‡ï¼‰
    fine_tune_optimizer = tf.keras.optimizers.AdamW(
        learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(
            initial_learning_rate=FINE_TUNE_LR,
            decay_steps=10000,
            decay_rate=0.9
        ),
        weight_decay=1e-4
    )
    
    model.compile(
        optimizer=fine_tune_optimizer,
        loss="categorical_crossentropy",
        metrics=["accuracy", tf.keras.metrics.Precision(name='precision'), 
                 tf.keras.metrics.Recall(name='recall')]
    )
    
    # ç»§ç»­å¾®è°ƒè®­ç»ƒ
    history_fine = model.fit(
        X_train, y_train,
        batch_size=BATCH_SIZE,
        epochs=BASE_EPOCHS + FINE_TUNE_EPOCHS,
        initial_epoch=history_base.epoch[-1],
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=1
    )
    
    # åˆå¹¶è®­ç»ƒå†å²ï¼ˆä¿ç•™åŸæ–‡ä»¶åï¼‰
    all_history = {
        "loss": history_base.history["loss"] + history_fine.history["loss"],
        "val_loss": history_base.history["val_loss"] + history_fine.history["val_loss"],
        "accuracy": history_base.history["accuracy"] + history_fine.history["accuracy"],
        "val_accuracy": history_base.history["val_accuracy"] + history_fine.history["val_accuracy"],
        "precision": history_base.history["precision"] + history_fine.history["precision"],
        "recall": history_base.history["recall"] + history_fine.history["recall"]
    }
    history_df = pd.DataFrame(all_history)
    history_df.to_csv(os.path.join(PROCESSED_DIR, "train_history.csv"), index=False)
    
    # æœ€ç»ˆè¯„ä¼°
    print("\nğŸ“Š è®­ç»ƒå®Œæˆ | v2.0æ¨¡å‹æœ€ç»ˆéªŒè¯é›†è¯„ä¼°ï¼š")
    val_loss, val_acc, val_precision, val_recall = model.evaluate(X_val, y_val, verbose=0)
    print(f"   - éªŒè¯é›†ç²¾åº¦ï¼š{val_acc:.4f}")
    print(f"   - éªŒè¯é›†æŸå¤±ï¼š{val_loss:.4f}")
    print(f"   - éªŒè¯é›†ç²¾ç¡®ç‡ï¼š{val_precision:.4f}")
    print(f"   - éªŒè¯é›†å¬å›ç‡ï¼š{val_recall:.4f}")
    
    print("\nğŸ“ v2.0ç‰ˆæœ¬ç”Ÿæˆæ–‡ä»¶æ¸…å•ï¼ˆä¿æŒåŸå‘½åï¼‰ï¼š")
    print(f"   âœ… æœ€ä½³æ¨¡å‹ï¼š{os.path.join(PROCESSED_DIR, 'best_tomato_model.keras')}")
    print(f"   âœ… è®­ç»ƒå†å²ï¼š{os.path.join(PROCESSED_DIR, 'train_history.csv')}")
    print(f"   âœ… TensorBoardæ—¥å¿—ï¼š{os.path.join(ROOT_DIR, 'logs', 'tomato_model')}")
    
    return model, history_base, history_fine

# ===================== ä¸»å‡½æ•°ï¼šå¯åŠ¨è®­ç»ƒ =====================
if __name__ == "__main__":
    if not os.path.exists(PROCESSED_DIR):
        raise FileNotFoundError(
            f"âŒ é¢„å¤„ç†ç›®å½•ä¸å­˜åœ¨ï¼š{PROCESSED_DIR}\n"
            "è¯·å…ˆæ‰§è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬ï¼špython src/data/prepare_tomato_data.py"
        )
    
    try:
        model, history_base, history_fine = train_tomato_model()
        print("\nğŸ‰ ç•ªèŒ„èƒè¿«è¯†åˆ«æ¨¡å‹ï¼ˆv2.0 | 20251210 | CBAMæ³¨æ„åŠ›æœºåˆ¶ç‰ˆï¼‰è®­ç»ƒå…¨éƒ¨å®Œæˆï¼")
    except Exception as e:
        print(f"\nâŒ v2.0æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å‡ºé”™ï¼š{str(e)}")
        raise